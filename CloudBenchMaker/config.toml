# CloudBenchMaker Configuration
# This config file maps CloudBenchMaker settings to the imsearch_benchmaker framework

# log level
log_level = "INFO"

#dataset description
benchmark_name = "CloudBench"
benchmark_description = "A benchmark for cloud image retrieval"
benchmark_author = "Francisco Lozano"
benchmark_author_email = "francisco.lozano@northwestern.edu"
benchmark_author_affiliation = "Northwestern University"
benchmark_author_orcid = "0009-0003-8823-4046"
benchmark_author_github = "FranciscoLozCoding"

# Column names (matching CloudBenchMaker structure)
column_image = "image"
column_image_id = "image_id"
column_mime_type = "mime_type"
column_license = "license"
column_doi = "doi"
column_query = "query_text"
column_query_id = "query_id"
column_relevance = "relevance_label"
column_tags = "tags"
column_summary = "summary"
column_confidence = "confidence"

# Image URL configuration
image_base_url = "https://web.lcrc.anl.gov/public/waggle/datasets/CloudBench/images"
image_url_temp_column = "image_url"

# File paths
image_root_dir = "/tmp/CloudBench/images"
meta_json = "/tmp/CloudBench/images/rights_map.json"
metadata_jsonl = "/tmp/CloudBench/images/metadata.jsonl"
images_jsonl = "/Volumes/data/inputs/images.jsonl"
seeds_jsonl = "/Volumes/data/inputs/seeds.jsonl"
annotations_jsonl = "/Volumes/data/outputs/annotations.jsonl"
query_plan_jsonl = "/Volumes/data/outputs/query_plan.jsonl"
qrels_jsonl = "/Volumes/data/outputs/cloudbench_qrels.jsonl"
qrels_with_score_jsonl = "/Volumes/data/outputs/cloudbench_qrels_with_similarity_score.jsonl"
summary_output_dir = "/Volumes/data/outputs/summary"
hf_dataset_dir = "/Volumes/data/outputs/hf_dataset"
hf_dataset_card_path = "/Users/franciscolozano/Documents/Github/imsearch_benchmarks/CloudBenchMaker/public/dataset_card.md"

# Query planning
query_plan_num_seeds = 120  # Number of seed images to use for query generation
query_plan_neg_total = 40
query_plan_neg_hard = 25
query_plan_neg_nearmiss = 10
query_plan_neg_easy = 5
query_plan_random_seed = 14
query_plan_seed_image_ids_column = "seed_image_ids"
query_plan_candidate_image_ids_column = "candidate_image_ids"

# Hugging Face configuration (sensitive fields use _ prefix)
_hf_repo_id = "sagecontinuum/CloudBench"
_hf_private = false

# Boolean columns
columns_boolean = [
    "occlusion_present",        # True if the cloud is partially occluded
    "multiple_cloud_types",     # True if multiple cloud types are visible in the image
    "horizon_visible",          # True if the horizon line is visible
    "ground_visible",           # True if ground or terrain is visible in the image
    "sun_visible",              # True if the sun is visible in the image
    "precipitation_visible",    # True if rain, snow, or other precipitation is visible
    "overcast",                 # True if the sky is completely overcast (no clear sky visible)
    "multiple_layers",          # True if multiple cloud layers are visible at different altitudes
    "storm_visible"             # True if a storm is visible (e.g., cumulonimbus with storm features)
]

# Taxonomy columns (locked taxonomy for CloudBench)
[columns_taxonomy]
viewpoint = [
    "ground_upward",     # Ground-based camera looking upward at sky
    "ground_horizontal", # Ground-based camera looking horizontally
    "fisheye_sky",       # Fisheye lens capturing full sky dome
    "oblique",           # Angled view (not straight up or horizontal)
    "other",             # Other viewpoint
    "unknown"            # Not possible to determine
]
lighting = [
    "day",           # Sunlit or daylight scenes
    "night",         # Low light, dark, artificial light
    "dusk",          # Twilight, sunset, sunrise
    "bright",        # High brightness, strong sunlight
    "overcast_light", # Overcast but still daylight
    "other",         # Other lighting
    "unknown"        # Not possible to determine
]
cloud_coverage = [
    "0%-25%",         # No clouds visible (clear sky)
    "25%-50%",     # Scattered clouds (25-50% coverage)
    "50%-75%",        # Broken clouds (50-90% coverage)
    "75%-100%",      # Overcast (90-100% coverage)
    "unknown"        # Not possible to determine
]
confounder_type = [
    "none",              # No confounders present, clear cloud visibility
    "fog",               # Fog present (can obscure or be confused with low clouds)
    "haze",              # Haze present (reduces visibility, can obscure clouds)
    "dust",              # Dust present (can look like clouds or obscure them)
    "smoke",             # Smoke present (can be confused with clouds)
    "sun_glare",         # Sun glare or lens flare obscuring clouds
    "precipitation",     # Precipitation (rain, snow) obscuring clouds
    "marine_layer",      # Marine layer or low-lying fog/stratus
    "industrial_plume", # Industrial plume or steam (can be confused with clouds)
    "multiple",          # Multiple confounders present
    "unknown"            # Not possible to determine
]

# Vision adapter configuration (OpenAI)
[vision_config]
adapter = "openai"
model = "gpt-5-mini"
system_prompt = """You are a atmospheric scientist labeling cloud images for an atmospheric science retrieval benchmark.
Output MUST be valid JSON matching the schema. Do not include extra keys.
Use the allowed taxonomy values exactly.
Be conservative: if unsure, choose 'unknown'.

Cloud classification guidance:
- Use standard cloud types: cirrus, cirrostratus, cirrocumulus, altocumulus, altostratus, cumulus, cumulonimbus, nimbostratus, stratocumulus, stratus, contrail
- Pay attention to cloud altitude (high: cirrus family, middle: alto family, low: stratus/cumulus)
- Distinguish between cloud types and weather phenomena (fog, haze, precipitation)

Tagging rules:
- Prefer tags that help retrieval and describe atmospheric conditions accurately
- Include tags that help with false-positive analysis (e.g., distinguishing clouds from fog, haze, or glare)
- Avoid redundant near-duplicates; pick the most specific tag
- Focus on atmospheric and meteorological features relevant to cloud science"""

user_prompt = """Analyze this cloud image and output JSON with the following fields.

Metadata context: The image has been labeled as {metadata.cloud_category}, use this as a guide.

Required fields:
1. summary: <= 30 words, factual description of the cloud(s) and atmospheric conditions. No speculation.

  2. Taxonomy fields (use exact values from allowed lists):
    - viewpoint: Camera perspective
    - lighting: Lighting conditions
    - cloud_coverage: Percentage of sky covered by clouds
    - confounder_type: Atmospheric phenomena that might obscure or be confused with clouds (none, fog, haze, dust, smoke, sun_glare, precipitation, marine_layer, industrial_plume, multiple, unknown)

3. Boolean fields (true/false):
   - occlusion_present: Cloud is partially occluded by other objects
   - multiple_cloud_types: Multiple distinct cloud types visible
   - horizon_visible: Horizon line is visible
   - ground_visible: Ground or terrain visible in the image
   - sun_visible: Sun is visible in the image
   - precipitation_visible: Rain, snow, or other precipitation visible
   - overcast: Sky is completely overcast (no clear sky visible)
   - multiple_layers: Multiple cloud layers at different altitudes visible
   - storm_visible: Storm features visible (e.g., cumulonimbus with storm characteristics)

4. tags: Choose 12-18 tags ONLY from the provided enum list. Focus on atmospheric features, cloud characteristics, and scene elements.

  5. confidence: 0.0-1.0 confidence score for each taxonomy field (viewpoint, lighting, cloud_coverage, confounder_type). Higher values indicate higher certainty.
"""
max_output_tokens = 6000
reasoning_effort = "low"
image_detail = "low"  # "low" saves tokens
max_images_per_batch = 900
completion_window = "24h"  # Completion window for the batch
vision_metadata_columns = ["cloud_category"]

# Pricing configuration (required for cost calculation)
# Prices are per million tokens. All values must be explicitly set.
price_per_million_input_tokens = 0.125  # Text input tokens
price_per_million_output_tokens = 1.00  # Text output tokens
price_per_million_cached_input_tokens = 0.0125  # Text cached input tokens
price_per_million_image_input_tokens = 0.125  # Image input tokens
price_per_million_image_output_tokens = 1.00  # Image output tokens

# Controlled tag vocabulary (atmospheric science and cloud-related)
min_tags = 14
max_tags = 25
controlled_tag_vocab = [
    # --- Cloud types (descriptive, not classification) ---
    "cirrus_clouds", "cirrostratus_clouds", "cirrocumulus_clouds",
    "altocumulus_clouds", "altostratus_clouds",
    "cumulus_clouds", "cumulonimbus_clouds", "nimbostratus_clouds",
    "stratocumulus_clouds", "stratus_clouds", "contrail",
    # --- Cloud characteristics / morphology ---
    "wispy_clouds", "feathery_clouds", "layered_clouds", "puffy_clouds", "fluffy_clouds",
    "billowing_clouds", "towering_clouds", "flat_clouds", "smooth_clouds", "textured_clouds",
    "thin_clouds", "thick_clouds", "dense_clouds", "sparse_clouds",
    "scattered_clouds", "patchy_clouds", "uniform_clouds",
    "high_altitude_clouds", "mid_altitude_clouds", "low_altitude_clouds",
    "multiple_layers", "single_layer",
    # --- Sky conditions ---
    "clear_sky", "partly_cloudy", "mostly_cloudy", "overcast", "variable_clouds",
    "blue_sky", "gray_sky", "white_sky", "colorful_sky",
    "horizon_visible", "no_horizon_visible",
    # --- Weather phenomena ---
    "precipitation_visible", "rain_visible", "snow_visible", "hail_visible",
    "storm_visible", "thunderstorm", "lightning_visible",
    "windy_conditions", "calm_conditions",
    "fog_present", "mist_present", "haze_present", "dust_visible",
    # --- Atmospheric features ---
    "sun_visible", "sun_partially_visible", "sun_occluded", "sunset", "sunrise",
    "moon_visible", "stars_visible",
    "rainbow_visible", "halo_visible", "sun_dogs_visible",
    "atmospheric_perspective", "distant_horizon", "infinite_sky",
    # --- Lighting conditions ---
    "bright_lighting", "dim_lighting", "backlit", "frontlit", "side_lit",
    "golden_hour", "blue_hour", "twilight",
    "high_contrast", "low_contrast", "even_lighting",
    "glare_present", "lens_flare", "sun_glare",
    # --- Terrain / landscape (when visible) ---
    "ground_visible", "terrain_visible", "landscape_visible",
    "mountain_visible", "hills_visible", "flat_terrain",
    "forest_visible", "grassland_visible", "desert_visible",
    "coastal", "ocean_visible", "water_body_visible",
    "urban_visible", "rural_visible", "buildings_visible",
    # --- Cloud dynamics / appearance ---
    "developing_clouds", "dissipating_clouds", "mature_clouds",
    "anvil_cloud", "shelf_cloud", "wall_cloud", "mammatus_clouds",
    "lenticular_clouds", "roll_clouds", "wave_clouds",
    "cloud_shadows", "cloud_gaps", "cloud_edges_visible",
    # --- Scene composition ---
    "sky_dominates", "clouds_dominate", "balanced_composition",
    "wide_sky_view", "narrow_sky_view", "full_sky_dome",
    "aerial_perspective", "ground_level_view",
    # --- Visibility / atmospheric clarity ---
    "clear_visibility", "reduced_visibility", "poor_visibility",
    "hazy_atmosphere", "clear_atmosphere", "turbid_air",
    # --- Time indicators (visual cues) ---
    "daytime", "nighttime", "dawn", "dusk", "midday",
    # --- Confounders (for false-positive analysis) ---
    "smoke_visible", "steam_visible", "industrial_plume",
    "cloud_like_feature", "not_clouds",
]

# Judge adapter configuration (OpenAI)
[judge_config]
adapter = "openai"
model = "gpt-5-mini"
system_prompt = """You are an atmospheric scientist creating and judging an image-retrieval benchmark for cloud images.
You will receive a query seed (1-3 seed images described by text) and a list of candidates.
The query should describe cloud types, atmospheric conditions, or meteorological features that would be useful for cloud image retrieval.
Queries should be specific enough to be meaningful but general enough to match multiple relevant images.
DO NOT use file names, dataset sources, IDs, or rights info for relevance.
Judge relevance ONLY from the summaries + metadata provided (including cloud_category, cloud_coverage, confounder_type, etc.).
Output MUST be valid JSON matching the schema.
Binary relevance only: 1 relevant, 0 not relevant.

Relevance guidelines:
- Consider cloud type, appearance, atmospheric conditions, and weather phenomena
- Distinguish between clouds and confounders (fog, haze, smoke, etc.) when relevant
- Consider cloud coverage, altitude, and morphology when evaluating relevance
- Be consistent: similar cloud types and conditions should receive similar relevance labels"""

user_prompt = """Tasks:
1) Write a realistic atmospheric scientist query_text (what someone would ask for in a search engine) describing the target cloud type, atmospheric condition, or meteorological feature. The query should be specific enough to be meaningful but general enough to match multiple relevant images.

2) Label each candidate image as relevant (1) or not relevant (0) to that query based on:
   - Cloud type and characteristics (cirrus, cumulus, stratus, etc.)
   - Atmospheric conditions (cloud coverage, weather phenomena)
   - Visual appearance and morphology
   - Presence of confounders (fog, haze, etc.) that might affect relevance

Be consistent with the query and try to make the query as concise as possible. Consider the metadata (cloud_category, cloud_coverage, confounder_type, boolean fields) when judging relevance."""
max_output_tokens = 8000
reasoning_effort = "medium"
max_queries_per_batch = 600
max_candidates = 100
completion_window = "24h"  # Completion window for the batch

# Pricing configuration (required for cost calculation)
# Prices are per million tokens. All values must be explicitly set.
price_per_million_input_tokens = 0.125  # Text input tokens 
price_per_million_output_tokens = 1.00  # Text output tokens
price_per_million_cached_input_tokens = 0.0125  # Text cached input tokens

# Similarity adapter configuration (CLIP)
[similarity_config]
adapter = "local_clip"
model = "apple/DFN5B-CLIP-ViT-H-14-378"
col_name = "clip_score"
device = "cpu"  # Device to run inference on 
use_safetensors = true  # Use safetensors format for model loading
